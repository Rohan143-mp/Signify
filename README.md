# âœ‹ğŸ¤Ÿ Signify â€” Real-Time Sign Language to Speech & Text

**Finalist project at Code Strom Hackathon 2025 | DIT University, Dehradun | Social Impact Theme**

---

## ğŸš€ Overview  
**Signify** is a real-time Sign Language Recognition System that bridges communication gaps for the deaf and mute community. It captures hand signs through a webcam, detects gestures for letters **Aâ€“Z** and special signs, and instantly translates them into text and speech â€” enabling seamless communication between signers and non-signers.

---

## ğŸ¯ Problem Statement  
Millions of deaf and mute individuals use sign language to express themselves, but most people around them canâ€™t understand it. This communication barrier creates daily challenges. **Signify** tackles this problem by providing an accessible, real-time translation system to promote inclusivity.

---

## ğŸ—ï¸ Key Features  
âœ… Real-time hand landmark detection using **MediaPipe**  
âœ… Custom-trained **Random Forest Classifier** on a self-built dataset  
âœ… Instant text and text-to-speech output  
âœ… Web-based system using **Flask**, accessible via any browser  
âœ… Plans for mobile app integration with **TensorFlow Lite**

---

## âš™ï¸ Tech Stack  

**Languages & Libraries:**  
- Python  
- OpenCV  
- MediaPipe  
- NumPy  
- scikit-learn  
- pyttsx3 (Text-to-Speech)  
- Flask (backend)  
- HTML, CSS, JavaScript (frontend)  
- Pickle (model serialization)

**Tools:**  
- VS Code (IDE)  
- Git & GitHub (version control)

---

## ğŸ“Š Dataset & Model  

- **Custom Dataset:** Captured using webcam and MediaPipe hand landmarks (21 points, flattened to 42 features per sample). Multiple samples per class for signs **Aâ€“Z** and special gestures.  
- **Preprocessing:** Normalized landmarks and labeled data points.  
- **Model:** RandomForestClassifier trained with scikit-learn and serialized as `model.p`.  
- **Innovation:** Entire dataset built in-house for relevance and accuracy without relying on pre-trained models.

---

## ğŸ–¥ï¸ How It Works  

1. Captures live webcam feed using **OpenCV**.  
2. Extracts hand landmarks in real time with **MediaPipe**.  
3. Normalizes and feeds landmarks into the trained model.  
4. Displays the predicted sign on screen and generates speech output.  
5. Accessible via a simple **Flask** web interface.

---

## ğŸ“± Future Scope  

âœ¨ **Mobile App:** Develop a responsive app with **TensorFlow Lite** + MediaPipe for on-device detection.  
âœ¨ **Two-Way Communication:** Add voice/text input to generate sign language animations (reverse translation).  
âœ¨ **Continuous Learning:** Integrate user feedback to improve prediction accuracy over time.  
âœ¨ **Regional Support:** Expand to multiple regional sign languages.

---

## ğŸ’™ Social Impact  
**Signify** was built under the **Social Impact** theme to empower the hearing and speech impaired community. It aims to reduce communication barriers and promote inclusivity in everyday life.
